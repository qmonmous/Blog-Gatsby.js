---
layout: post
title: üá´üá∑ Introduction au Web Mining (Scraping, Crawling) ‚Äì WMXP (Part. 1)
author: Quentin Monmousseau
tags: [Data Sc. & A.I.]
image: images/header-articles.png
date: 2019-05-07T23:46:37.121Z
draft: false
---

*Cet article vise √† introduire √† la collecte de donn√©es web, ou Web Mining, gr√¢ce au Scraping et au Crawling.*

---

**‚Äî Sommaire**

**[I. Le Scraping](#one)**  
**[II. Le Crawling](#two)**  

---

## Intro.

Le *Web Mining* est une branche sp√©cifique du Data Mining qui s'int√©resse aux donn√©es issues du web.

Ces donn√©es sont tr√®s int√©ressantes pour des probl√©matiques li√©es au marketing ou aux sciences humaines. En effet, elles permettent de contourner les biais des m√©thodes exp√©rimentales (pr√©con√ßus, orientations des questions...) - ou encore le *paradoxe de Labov* - car contrairement aux donn√©es recueillies lors de questionnaires/entretiens, les personnes produisent les contenus spontan√©ment.

En Web Mining, on analyse g√©n√©ralement trois types de donn√©es :
- Contenus du web avec le *Scraping*,
- Structure du web avec le *Crawling*,
- mais Usages du web (parcours utilisateurs, web analytics...), qu'on ne verra pas dans cet article,

### La limite des APIs web

Sur la plupart des gros sites web, des *APIs* sont mises √† disposition pour pouvoir en r√©cup√©rer les donn√©es. C'est par exemple le cas pour Twitter, Facebook ou Linkedin. Toutefois, certaines de ces APIs sont largement limit√©es (contenu innaccessible, rate limit, options payantes...). Autre probl√®me, des sites moins importants, mais tout aussi int√©ressant √† analyser, ne poss√®dent tout simplement pas d'API.
Pour r√©colter l'ensemble des donn√©es qui nous int√©ressent, il existe une solution puissante qui s'affranchit de ces limites : le *Scraping*.

---

<a id="one"></a>
## I. Le Scraping

*To scrape*, ou gratter en fran√ßais, c'est l‚Äôart d‚Äôextraire des donn√©es directement depuis leur repr√©sentation visuelle. Lorsqu'on cr√©e une page web, on va structurer son contenu avec du HTML. Du templating va permettre de formater et d'ins√©rer des donn√©es dynamiques dans ces structures. Ainsi, dans le cas des pages web, on va scraper le HTML lui-m√™me pour r√©cup√©rer pr√©cis√©ment les donn√©es souhait√©es. D'une certaine mani√®re, on proc√®de √† une r√©tro-ing√©nierie du processus de templating.

Pour commencer cette introduction au scraping, assurez-vous d'avoir installer les deux d√©pendances suivantes :
- <code>pip install requests</code>
- <code>pip install beautifulsoup4</code>

Nous allons commencer par r√©cup√©rer le contenu HTML de la page web qui nous int√©resse.

```python
# Import librairies
import requests # Requ√™tes aux pages web
from bs4 import BeautifulSoup # Scraping

# Requ√™te √† la page web
res = requests.get("http://www.echojs.com/")

# R√©cup√©ration du contenu HTML de la page
soup = BeautifulSoup(res.text, 'html.parser')
print(soup)
```

Notre objectif est maintenant de r√©cup√©rer les titres d'articles et leurs liens, contenus dans les balises <code>a</code>. Pour cela, on peut pr√©ciser le chemin qui m√®ne √† la balise qui nous int√©resse √† l'aide de la fonction <code>.select</code>.

```python
# S√©lection de la balise
links = soup.select('article > h2 > a')
```

Une fois s√©lectionn√©s, on va stocker les couples titre/lien dans une liste en les ajoutant un par un avec la fonction <code>.append</code>. Pour pr√©ciser le contenu qui nous int√©resse dans la balise cibl√©e, on utilise les fonctions <code>.get_text()</code> et <code>.get('href')</code> de BS4.

```python
# Initialisation d'une liste
data = []

# Remplissage de la liste avec les couples title/url
for link in links:
    data.append({
        'title': link.get_text(),
        'url': link.get('href')
    })

# Affichage de la liste
for couple in data:
    print('Name: {}\nLink: {}\n'.format(couple['title'], couple['url']))
```

Maintenant qu'on a r√©ussi √† r√©cup√©rer les titres des articles et leur url, il ne nous reste plus qu'√† stocker nos donn√©es. Ici dans un fichier .csv...

```python
import csv

# Ouverture d'un fichier csv
with open('./scraping.csv', 'w') as f:

    # Ecriture des intitul√©s des colonnes
    writer = csv.DictWriter(f, fieldnames=['title', 'url'])
    writer.writeheader()

    # Ecriture de chaque couple titre/url dans le fichier .csv
    for couple in data:
        writer.writerow(couple)
```

...ou ici au format .json.

```python
import json

# Initialisation d'un dictionnaire
data2 = []

# Remplissage du dictionnaire
for link in links:
    data2 = {
        'title': link.get_text(),
        'url': link.get('href')
    }

# Ouverture d'un fichier json et compl√©tion
with open('./scraping.json', 'w') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

---

<a id="two"></a>
## II. Le Crawling

*To crawl*, ou ramper en fran√ßais, c'est parcourir le web √† l‚Äôaide d‚Äôun programme (bot, spider) en r√©cup√©rant :
- les pages parcourues
- les liens entre elles

Si vous √™tes famili√©s avec la notion de graphe, il appara√Æt √©vident qu'on constitue alors un r√©seau de pages qu'on va pouvoir analyser ([ici avec NetworkX](https://github.com/qmonmous/DataScience-X-Python/blob/master/Bonus2.%20Introduction%20%C3%A0%20l'analyse%20de%20r%C3%A9seau%20avec%20NetworkX/Th%C3%A9orie%20des%20graphes%20et%20Analyse%20de%20r%C3%A9seau.ipynb)).

Pour commencer cette introduction au crawling, assurez-vous d'avoir installer les d√©pendances suivantes :
- <code>pip install requests</code>
- <code>pip install urllib</code>
- <code>pip install beautifulsoup4</code>
- <code>pip install networkx</code>


```python
import requests # Requ√™tes aux pages web
from bs4 import BeautifulSoup # Scraping
from urllib.parse import urljoin # Reconstruction d'URLs (pour les relatives)
import networkx as nx # G√©n√©ration et analyse de r√©seau

# URLs de d√©part du crawl
START_URLS = [
    'https://quentin-monmousseau.netlify.com/'
]

# Initialisation du r√©seau de page g√©n√©r√© par le crawl
graph = nx.DiGraph() # DiGraph : r√©seau dirig√© (une url pointe vers une autre dans un seul sens)

# Profondeur souhait√©e pour le r√©seau
MAX_DEPTH = 1

# Initialisation d'une liste contenant les URLs de d√©part
queue = []

for url in START_URLS:
    queue.append((url, 0))

```

```python
# On boucle sur la liste d'URLs
while len(queue):

    # Affichage de l'URL visit√© et de sa profondeur
    url, depth = queue.pop(0)
    print('Level [%i]: %s' % (depth, url))

    # Requ√™tage √† la page web
    r = requests.get(url)

    # Si la requ√™te √©choue (code > 400), on poursuit le crawl
    if r.status_code >= 400:
        continue

    # Si la requ√™te ne renvoie pas de HTML, on poursuit le crawl
    if 'html' not in r.headers['content-type']:
        continue

    # R√©cup√©ration du contenu HTML de la page
    soup = BeautifulSoup(r.text, 'html.parser')
```

```python
    # On parcourt les liens des pages
    for link in soup.find_all('a'):
        new_url = link.get('href')
```

Les pages n'√©tant pas toujours structur√©es correctement, il faut √©viter certaines erreurs qui pourraient d√©grader la qualit√© de notre r√©seau.  
On va sauter les balises dont le contenu pose probl√®me : href inexistant, ancre interne (#) qui nous am√®nerait √† revisiter la m√™me page, lien vers un mail...

```python
        # Si la balise a ne contient pas de href, on poursuit le crawl
        if new_url is None:
            continue

        # Si le href commence par '#', on poursuit le crawl
        if new_url.startswith('#'):
            continue

        # Si le lien commence par 'javascript:' ou 'mailto:', on poursuit le crawl
        if new_url.startswith('javascript:') or new_url.startswith('mailto:'):
            continue
```

```python
        # Si la nouvelle URL est relative, on la reconstruit en ajoutant l'URL pr√©c√©dent au href
        target_url = urljoin(url, new_url)

        # Ajout du nouvel URL au r√©seau (URL d'origine vers nouveau URL)
        graph.add_edge(url, target_url)

        # D√©j√† crawl√© si pr√©sent dans le r√©seau au m√™me degr√© (profondeur)
        already_crawled = target_url in graph and graph.out_degree(target_url) > 0

        # Si on est toujours sous la profondeur souhait√©e et que la page n'est pas d√©j√† crawl√©e...
        if depth < MAX_DEPTH and not already_crawled:

            # ...Ajout de l'URL √† la liste √† parcourir
            queue.append((target_url, depth + 1))
```


```python
# Affichage des informations du r√©seau
print(nx.info(graph))

# Affichage du r√©seau
nx.draw(graph, with_labels=False, node_size=10)
```

![](images/r√©seau.png)

### Limites de la visualisation avec NetworkX

La librairie *NetworkX* permet de faire de nombreux calculs statistiques utiles pour √©tudier le r√©seau. Toutefois, elle ne permet pas de le visualiser confortablement. Pour se faire, on lui pr√©f√®re des outils tels que *Gephi* (un peu vieillot) ou *Manylines* (d√©velopp√© r√©cemment par le MediaLab de SciencesPo Paris) par exemple.  
Pour analyser notre r√©seau √† l'aide de ces outils, il est n√©cessaire de l'exporter au format .gexf.

```python
# Export du r√©seau au format .gexf
nx.write_gexf(graph, 'crawling.gexf')
```