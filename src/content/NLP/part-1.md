---
layout: post
title: üá´üá∑ Introduction au NLP - NLPXP (Part. 1) / En cours d'√©criture
author: Quentin Monmousseau
tags: [Data Sc. & A.I]
image: images/header-articles.png
date: 2019-05-20T23:46:37.121Z
draft: false
---

*Dans cet article, nous d√©couvrons l'analyse textuelle √† l'aide des librairies NLTK, spaCy et Gensim, ainsi que les mod√©lisations possibles en NLP.*

---

**‚Äî Sommaire**

**[I. Du texte √† la matrice](#one)**  

**[II. Preprocessing](#two)**  
‚Ä¢ [A. R√©cup√©ration du corpus](#two-a)  
‚Ä¢ [B. Nettoyage du corpus](#two-b)

**[III. Modeling](#three)**  
‚Ä¢ [A. Mod√©lisation automatique de sujet : LDA, NMF](#three-a)  

---

## Intro.

Le *Natural Language Processing* (ou *NLP*) est un ensemble de m√©thodes informatiques permettant de <mark>comprendre ou d'analyser le langage humain</mark>. On parle de *corpus* pour d√©signer l'ensemble des documents analys√©s.

On peut avoir une approche purement statistique (on parle alors de lexicom√©trie) en √©tudiant des donn√©es telles que les occurrences des mots par exemple.  
Plus largement, le NLP va permettre de mod√©liser des motifs dans le texte en s'int√©ressant √† la co-occurence de certains mots par exemple. Ainsi, si il est extr√™mement difficile de trouver des r√®gles tout √† fait g√©n√©rales en NLP, le Machine Learning va permettre de les d√©couvrir et de les appliquer aux corpus suivants.

![](images/whatisnlp.jpg)
###### Looks fascinating, right?

Voici, entre autres, quelques sujets sur lesquels travailler avec du NLP :
- l‚Äôanalyse de sentiments,
- la mod√©lisation de th√©matiques,
- la traduction automatique...

---

## I. Du texte √† la matrice

Comme introduit dans mon article sur le Machine Learning, il est n√©cessaire de repr√©senter nos donn√©es sous forme de matrices pour - *in fine* - leur appliquer des mod√®les pr√©dictifs.
Prenons le corpus suivant : <code>"Je suis un homme. Un homme souvent paradoxal."</code>.  
Il est possible de repr√©senter ce corpus sous forme de vecteur. A chaque fois qu‚Äôun nouveau mot appara√Æt, il est ajout√© au vecteur :
<code>[je suis un homme souvent paradoxal]</code>

Pour d√©crire une phrase, on va ensuite attribuer 1 si le mot est pr√©sent, 0 sinon.
<code>"Un homme souvent paradoxal"</code> devient <code>[0 0 1 1 1 1]</code>.

Par ailleurs, forcer le vecteur √† attribuer une position fixe √† chaque mot commence √† r√©pondre √† une probl√©matique inh√©rente au NLP. Plus le corpus est grand, plus le vecteur va contenir de mots et donc devenir co√ªteux d‚Äôun point de vue calcul machine. Il existe donc des m√©thodes permettant, par exemple, de regrouper les mots sous la forme de leur radical : <code>[d√©couper, coupant, couperaient]</code> deviennent <code>[coupe]</code>. Cela permet de limiter le temps de calcul tout en conservant (g√©n√©ralement) le sens des mots. Mais passons √† la pratique.

---

## II. Preprocessing

Dans cet article, j'ai choisi de travailler avec les librairies *NLTK, spaCy et Gensim* pour ouvrir le champs des possibles. La plupart des √©tapes peuvent √™tre r√©alis√©es √† l'aide des 3 librairies, ici l'id√©e est surtout de comprendre l'int√©r√™t des diff√©rentes m√©thodes applicables en NLP.

### A. R√©cup√©ration du corpus
La premi√®re √©tape consiste √† r√©cup√©rer le corpus sur lequel on souhaite travailler. Pour se faire, il existe diff√©rentes m√©thodes qui ne seront pas d√©crites en d√©tail dans cet article.  
On pourrait par exemple r√©cup√©rer des fichiers de donn√©es sur des plateformes opendata (comme Kaggle) ou scraper des pages web pour en extraire le contenu (voir mon article sur le web mining).  
Pour avoir une premi√®re intuition des fonctions que nous allons utiliser, prenons les phrases suivantes en guise de corpus : 

```python
# Import de NLTK, librairie phare du NLP
import nltk
nltk.download('punkt')

# G√©n√©ration du corpus
corpus = "Ceci est une introduction √† l‚Äôanalyse textuelle.¬†Do$nt l'analyse pour‚Äôrait po$$ser probl√®me." 
```

On va maintenant se rapprocher de la repr√©sentation vectorielle attendue. Pour se faire, on peut *tokenizer* notre corpus.

- **La *tokenization*** : permet le d√©coupage en mots des diff√©rents documents qui constituent le corpus.

```python
# Tokenization en vecteur de mots
print(nltk.word_tokenize(corpus))
```

Pour r√©duire la longueur de ce vecteur, nous allons utiliser la m√©thode du *bag-of-words*.

- **Le *bag-of-words*** : Pour que chaque occurence de mots ne viennent pas agrandir le vecteur, nous les regroupons en les ajoutant √† leur premi√®re apparition. Cela va former un vecteur dont la longueur est <mark>l'ensemble du vocabulaire</mark> (c'est-√†-dire des mots diff√©rents) <mark>dans l'ensemble des documents du corpus</mark>. Chaque document peut maintenant √™tre d√©crit par ce m√™me vecteur [a b c ...] avec a, b, c, ... le nombre d'occurences ‚Äì dans le document ‚Äì du mot repr√©sent√©.

### B. Nettoyage du corpus
La deuxi√®me √©tape consiste √† nettoyer le texte pour le pr√©parer √† l‚Äôanalyse.

Il s'agit de **normaliser** le vecteur de mots dans le but de supprimer des d√©tails inutiles au dictionnaire qu‚Äôon souhaite construire (ponctuation, caract√®res sp√©ciaux, conjugaison, mots ayant trop de poids dans le cadre d‚Äôun mod√®le pr√©dictif de th√©matiques‚Ä¶). Pour cela, voici les m√©thodes les plus commun√©ment appliquer.

- **Les *Regex*** : (ou expressions r√©guli√®res) qui permettent de filtrer les cha√Ænes de caract√®res.

```python
# Application d‚Äôune regex pour se d√©barrasser des caract√®res inutiles
tokenizer = nltk.RegexpTokenizer(r'\w+')
print(tokenizer.tokenize(corpus))
```

- **Suppression des *stop-words***¬†: qui sont les mots tr√®s courants dans la langue √©tudi√©e ("et", "√†", "le"... en fran√ßais) et qui n'apportent pas r√©ellement de valeur informative pour la compr√©hension du sens du texte.

- **Suppression des mots les plus fr√©quents**¬†: pour ne garder que les mots qui permettent de distinguer le contenu d'un texte d'un autre.

- **La *lemmatisation*** : qui consiste √† repr√©senter les mots sous leur forme canonique. Les verbes conjugu√©s vont √™tre pass√©s √† l'infinitif, les noms au masculin singulier...

- **Le *stemming*** :¬†qui permet de supprimer les pr√©fixes/suffixes connus pour ne garder que la racine (donc la partie la plus porteuse de sens) des mots.

---

## Processing

- **Les *n-grams*** : sous-s√©quence de *n* √©l√©ments construite √† partir d‚Äôune s√©quence donn√©e. La probabilit√© des √©l√©ments dans le texte est alors d√©pendante des √©l√©ments pr√©c√©dents et suivants. Cela permet notamment de prendre en compte une partie du contexte de la phrase.  
Par exemple, dans <code>"mourir de rire"</code> et <code>"mourir de faim"</code>, <code>"mourir"</code> ne v√©hicule pas la m√™me id√©e (elle est positive dans le premier cas, n√©gative dans le second).

- **Le *POS tagging* (Part of Speech tagging)** : qui associe aux mots d'une phrase leurs informations grammaticales.

...

---

## Modeling

### Mod√©lisation automatique de sujet : LDA, NMF

L'objectif de ce type de mod√©lisation de sujets est de r√©cup√©rer de potentielles cat√©gories pour des traitements ult√©rieurs. Cette mod√©lisation offre surtout une meilleure compr√©hension de la structuration du texte en vue de la cr√©ation de features (mettre l'accent sur certains mots, comprendre ce qui d√©finit une cat√©gorie, etc.).

```python
# Mod√©lisation automatique de sujet avec le mod√®le LDA
from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=20,
                                max_iter=5,
                                learning_method='online',
                                learning_offset=50,
                                random_state=0)
                                .fit(tf)
```

...